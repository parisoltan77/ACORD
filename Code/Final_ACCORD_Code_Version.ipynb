{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1hrFMQQY3aL"
      },
      "outputs": [],
      "source": [
        "!pip install datasets scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvJ9e-oUZCAO"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_SCl3BoZD-v"
      },
      "outputs": [],
      "source": [
        "label_mapping = { 0: 'Cause-Effect-e2-e1', 1: 'Cause-Effect-e1-e2', 2: 'Component-Whole', 3: 'Component-Whole', 4: 'Content-Container', 5: 'Content-Container', 6: 'Entity-Destination', 7: 'Entity-Destination', 8: 'Entity-Origin', 9: 'Entity-Origin', 10: 'Instrument-Agency', 11: 'Instrument-Agency', 12: 'Member-Collection', 13: 'Member-Collection', 14: 'Message-Topic', 15: 'Message-Topic', 16: 'Product-Producer', 17: 'Product-Producer', 18: 'Other' }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXNCkwyGZKFW"
      },
      "source": [
        "**ANOVA Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdRKd4VHZG7l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # دانلود منابع punkt_tab برای رفع خطا\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def to_lowercase(self, input_text):\n",
        "        return input_text.lower()\n",
        "\n",
        "    def remove_punctuation(self, input_text):\n",
        "        return input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    def remove_whitespace(self, input_text):\n",
        "        return ' '.join(input_text.split())\n",
        "\n",
        "    def remove_stopwords(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(word for word in words if word not in self.stop_words)\n",
        "\n",
        "    def stem_text(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(self.stemmer.stem(word) for word in words)\n",
        "\n",
        "    def lemmatize_text(self, input_text):\n",
        "        doc = self.nlp(input_text)\n",
        "        return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "    def remove_special_characters(self, input_text):\n",
        "        return re.sub(r'[^A-Za-z\\s]', ' ', input_text)\n",
        "\n",
        "    def tokenize_text(self, input_text):\n",
        "        return nltk.word_tokenize(input_text)\n",
        "\n",
        "    def preprocess(self, input_text, with_tokenize=False):\n",
        "        input_text = self.to_lowercase(input_text)\n",
        "        input_text = self.remove_punctuation(input_text)\n",
        "        input_text = self.remove_special_characters(input_text)\n",
        "        input_text = self.remove_whitespace(input_text)\n",
        "        input_text = self.remove_stopwords(input_text)\n",
        "        if with_tokenize:\n",
        "            input_text = self.tokenize_text(input_text)\n",
        "        return input_text\n",
        "\n",
        "# تعریف کلاس AnoVaText\n",
        "class AnoVaText:\n",
        "    def __init__(self, texts, labels, num_important_words):\n",
        "        if len(texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_words = self.extract_unique_words(texts)\n",
        "        self.word_counts = self.compute_word_counts(texts)\n",
        "\n",
        "    def extract_unique_words(self, texts):\n",
        "        words = []\n",
        "        for text in texts:\n",
        "            words.extend(text.split())\n",
        "        return list(set(words))\n",
        "\n",
        "    def compute_word_counts(self, texts):\n",
        "        word_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            word_counts.append(count)\n",
        "        return word_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_words = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_word_counts = [self.word_counts[i] for i in label_indices]\n",
        "            other_word_counts = [self.word_counts[i] for i in other_indices]\n",
        "\n",
        "            word_scores = {}\n",
        "            for word in self.unique_words:\n",
        "                label_word_frequencies = [count.get(word, 0) for count in label_word_counts]\n",
        "                other_word_frequencies = [count.get(word, 0) for count in other_word_counts]\n",
        "\n",
        "                if np.var(label_word_frequencies) > 0 and np.var(other_word_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_word_frequencies, other_word_frequencies)\n",
        "                    word_scores[word] = (f_stat, np.var(label_word_frequencies))\n",
        "                else:\n",
        "                    word_scores[word] = (0, np.var(label_word_frequencies))\n",
        "\n",
        "            sorted_words = sorted(word_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_words[label] = sorted_words[:self.num_important_words]\n",
        "\n",
        "        return important_words\n",
        "\n",
        "# بارگیری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "print(\"length \", len(train_data))\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "txt_prs = TextPreprocessor()\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # حذف `and` و کلمه بعد از `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # استخراج متن بین تگ‌ها و حذف فاصله‌های اضافی\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(txt_prs.preprocess(text, with_tokenize=False))  # پیش‌پردازش متن\n",
        "        # شرط برچسب‌ها\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# شمارش نمونه‌های هر دسته\n",
        "cause_effect_count = target_list.count(\"cause-effect\")\n",
        "others_count = target_list.count(\"others\")\n",
        "\n",
        "print(f\"تعداد نمونه‌های cause-effect: {cause_effect_count}\")\n",
        "print(f\"تعداد نمونه‌های others: {others_count}\")\n",
        "\n",
        "# ایجاد یک شیء از TextAnalyzerANOVA\n",
        "analyzer = AnoVaText(text_list, target_list, 50)\n",
        "important_words = analyzer.analyze()\n",
        "\n",
        "# خروجی به فایل اکسل\n",
        "with pd.ExcelWriter('important_words.xlsx') as writer:\n",
        "    for key, value in important_words.items():\n",
        "        df = pd.DataFrame([(word, score[0], score[1]) for word, score in value], columns=['Word', 'Score', 'Variance'])\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words have been written to the Excel file 'important_words.xlsx'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S0zuCmTZOnC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# تعریف کلاس AnoVaTextBigram\n",
        "class AnoVaTextBigram:\n",
        "    def __init__(self, bigram_texts, labels, num_important_words):\n",
        "        if len(bigram_texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = bigram_texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_bigrams = self.extract_unique_bigrams(bigram_texts)\n",
        "        self.bigram_counts = self.compute_bigram_counts(bigram_texts)\n",
        "\n",
        "    def extract_unique_bigrams(self, texts):\n",
        "        bigrams = []\n",
        "        for text in texts:\n",
        "            bigrams.extend(text.split())\n",
        "        return list(set(bigrams))\n",
        "\n",
        "    def compute_bigram_counts(self, texts):\n",
        "        bigram_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            bigram_counts.append(count)\n",
        "        return bigram_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_bigrams = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_bigram_counts = [self.bigram_counts[i] for i in label_indices]\n",
        "            other_bigram_counts = [self.bigram_counts[i] for i in other_indices]\n",
        "\n",
        "            bigram_scores = {}\n",
        "            for bigram in self.unique_bigrams:\n",
        "                label_bigram_frequencies = [count.get(bigram, 0) for count in label_bigram_counts]\n",
        "                other_bigram_frequencies = [count.get(bigram, 0) for count in other_bigram_counts]\n",
        "\n",
        "                if np.var(label_bigram_frequencies) > 0 and np.var(other_bigram_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_bigram_frequencies, other_bigram_frequencies)\n",
        "                    bigram_scores[bigram] = (f_stat, np.var(label_bigram_frequencies))\n",
        "                else:\n",
        "                    bigram_scores[bigram] = (0, np.var(label_bigram_frequencies))\n",
        "\n",
        "            sorted_bigrams = sorted(bigram_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_bigrams[label] = sorted_bigrams[:self.num_important_words]\n",
        "\n",
        "        return important_bigrams\n",
        "\n",
        "# بارگیری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # حذف `and` و کلمه بعد از `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # استخراج متن بین تگ‌ها و حذف فاصله‌های اضافی\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(preprocess_text(text))  # پیش‌پردازش متن\n",
        "\n",
        "        # شرط برچسب‌ها\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# تولید bigram ها از رشته‌ها\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
        "bigram_features = vectorizer.fit_transform(text_list)\n",
        "bigram_terms = vectorizer.get_feature_names_out()\n",
        "bigram_terms = [term.replace(' ', '_') for term in bigram_terms]\n",
        "\n",
        "# ساخت دیتافریم از bigram ها\n",
        "bigram_data = [' '.join([bigram_terms[i] for i in row.nonzero()[1]]) for row in bigram_features]\n",
        "df = pd.DataFrame({'bigram_text': bigram_data, 'label': target_list})\n",
        "\n",
        "# ایجاد یک شیء از TextAnalyzerANOVA با bigram ها\n",
        "analyzer = AnoVaTextBigram(df['bigram_text'], df['label'], 50)\n",
        "important_bigrams = analyzer.analyze()\n",
        "\n",
        "# خروجی به فایل اکسل\n",
        "with pd.ExcelWriter('important_bigrams.xlsx') as writer:\n",
        "    for key, value in important_bigrams.items():\n",
        "        # تبدیل نتیجه به دیتافریم\n",
        "        df = pd.DataFrame([(bigram, score[0], score[1]) for bigram, score in value], columns=['Bigram', 'Score', 'Variance'])\n",
        "        # نوشتن دیتافریم به صفحه اکسل\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important bigrams have been written to the Excel file 'important_bigrams.xlsx'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRa3Q8khZTDs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ترکیب نتایج `unigram` و `bigram`\n",
        "combined_results = {}\n",
        "\n",
        "# لیبل‌ها\n",
        "labels = ['cause-effect', 'others']\n",
        "\n",
        "for key in labels:\n",
        "    if key in important_words:\n",
        "        unigram_data = important_words[key]\n",
        "        unigram_df = pd.DataFrame(unigram_data, columns=['Term', 'Values'])\n",
        "        unigram_df[['Score', 'Variance']] = pd.DataFrame(unigram_df['Values'].tolist(), index=unigram_df.index)\n",
        "        unigram_df = unigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        unigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    if key in important_bigrams:\n",
        "        bigram_data = important_bigrams[key]\n",
        "        bigram_df = pd.DataFrame(bigram_data, columns=['Term', 'Values'])\n",
        "        bigram_df[['Score', 'Variance']] = pd.DataFrame(bigram_df['Values'].tolist(), index=bigram_df.index)\n",
        "        bigram_df = bigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        bigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    # ترکیب دو دیتافریم\n",
        "    combined_df = pd.concat([unigram_df, bigram_df], ignore_index=True)\n",
        "\n",
        "    # مرتب‌سازی نتایج بر اساس Score\n",
        "    combined_df = combined_df.sort_values(by='Score', ascending=False)\n",
        "\n",
        "    # ذخیره نتایج ترکیبی در دیکشنری\n",
        "    combined_results[key] = combined_df\n",
        "\n",
        "# ذخیره نتایج در فایل اکسل با برگه‌های جداگانه برای هر کلاس لیبل\n",
        "with pd.ExcelWriter('important_words_and_bigrams_combined.xlsx') as writer:\n",
        "  for key, df in combined_results.items():\n",
        "    df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words and bigrams have been written to the Excel file 'important_words_and_bigrams_combined.xlsx'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f13cBflZW65"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# گام ۱: بارگذاری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# گام ۲: استخراج کلمات مهم از فایل اکسل\n",
        "cause_effect_df = pd.read_excel('important_words_and_bigrams_combined.xlsx', sheet_name='cause-effect')\n",
        "important_terms = set(cause_effect_df['Term'].tolist())\n",
        "\n",
        "# گام ۳: آماده‌سازی داده‌ها\n",
        "def extract_entities(sentence):\n",
        "    try:\n",
        "        entity1 = re.search(r\"<e1>(.*?)</e1>\", sentence).group(1)\n",
        "        entity2 = re.search(r\"<e2>(.*?)</e2>\", sentence).group(1)\n",
        "        return entity1, entity2\n",
        "    except AttributeError:\n",
        "        return \"\", \"\"\n",
        "\n",
        "def get_terms_between_entities(sentence):\n",
        "    entity1, entity2 = extract_entities(sentence)\n",
        "    if entity1 and entity2:\n",
        "        terms_between_entities = sentence[sentence.index(entity1) + len(entity1) : sentence.index(entity2)].split()\n",
        "        return ' '.join(terms_between_entities)\n",
        "    return \"\"\n",
        "\n",
        "# ایجاد لیست‌های یکتا برای داده‌های آموزشی\n",
        "train_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(train_data, train_data['relation'])))\n",
        "train_texts, train_labels = zip(*train_texts_labels)\n",
        "\n",
        "# ایجاد لیست‌های یکتا برای داده‌های تست\n",
        "test_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(test_data, test_data['relation'])))\n",
        "test_texts, test_labels = zip(*test_texts_labels)\n",
        "\n",
        "# متعادل‌سازی داده‌های آموزشی\n",
        "cause_effect_train = [(text, label) for text, label in zip(train_texts, train_labels) if label == 'cause-effect']\n",
        "others_train = [(text, label) for text, label in zip(train_texts, train_labels) if label == 'others']\n",
        "\n",
        "min_train_samples = min(len(cause_effect_train), len(others_train))\n",
        "balanced_train_data = cause_effect_train[:min_train_samples] + others_train[:min_train_samples]\n",
        "\n",
        "# متعادل‌سازی داده‌های تست\n",
        "cause_effect_test = [(text, label) for text, label in zip(test_texts, test_labels) if label == 'cause-effect']\n",
        "others_test = [(text, label) for text, label in zip(test_texts, test_labels) if label == 'others']\n",
        "\n",
        "min_test_samples = min(len(cause_effect_test), len(others_test))\n",
        "balanced_test_data = cause_effect_test[:min_test_samples] + others_test[:min_test_samples]\n",
        "\n",
        "# جدا کردن متن‌ها و برچسب‌ها\n",
        "balanced_train_texts, balanced_train_labels = zip(*balanced_train_data)\n",
        "balanced_test_texts, balanced_test_labels = zip(*balanced_test_data)\n",
        "\n",
        "# تبدیل کلمات مهم به یک وکتور\n",
        "vectorizer = CountVectorizer(vocabulary=important_terms, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(balanced_train_texts).toarray()\n",
        "X_test = vectorizer.transform(balanced_test_texts).toarray()\n",
        "\n",
        "# تبدیل برچسب‌ها به آرایه numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# گام ۵: ایجاد مدل انسامبل\n",
        "logreg = LogisticRegression()\n",
        "rf = RandomForestClassifier()\n",
        "gb = GradientBoostingClassifier()\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('lr', logreg),\n",
        "    ('rf', rf),\n",
        "    ('gb', gb)\n",
        "], voting='hard')  \n",
        "\n",
        "# گام ۶: آموزش مدل\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# گام ۷: پیش‌بینی و ارزیابی مدل\n",
        "predictions = ensemble.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions)\n",
        "\n",
        "# چاپ نتایج تحلیل\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "\n",
        "# گام ۹: ترسیم ماتریس درهم‌ریختگی\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['cause-effect', 'others'], yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
}
