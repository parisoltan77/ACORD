{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sANeJJKayaQH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets scikit-learn lime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "oRDyUaoJznYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "CstTVPgszrAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install fsspec==2023.9.2"
      ],
      "metadata": {
        "id": "l_Yyajba0Qef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = { 0: 'Cause-Effect-e2-e1', 1: 'Cause-Effect-e1-e2', 2: 'Component-Whole', 3: 'Component-Whole', 4: 'Content-Container', 5: 'Content-Container', 6: 'Entity-Destination', 7: 'Entity-Destination', 8: 'Entity-Origin', 9: 'Entity-Origin', 10: 'Instrument-Agency', 11: 'Instrument-Agency', 12: 'Member-Collection', 13: 'Member-Collection', 14: 'Message-Topic', 15: 'Message-Topic', 16: 'Product-Producer', 17: 'Product-Producer', 18: 'Other' }"
      ],
      "metadata": {
        "id": "IXFEQHRyzulc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA Method**"
      ],
      "metadata": {
        "id": "a0XY_X1DzwKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ù†Ø§Ø¨Ø¹ punkt_tab Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def to_lowercase(self, input_text):\n",
        "        return input_text.lower()\n",
        "\n",
        "    def remove_punctuation(self, input_text):\n",
        "        return input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    def remove_whitespace(self, input_text):\n",
        "        return ' '.join(input_text.split())\n",
        "\n",
        "    def remove_stopwords(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(word for word in words if word not in self.stop_words)\n",
        "\n",
        "    def stem_text(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(self.stemmer.stem(word) for word in words)\n",
        "\n",
        "    def lemmatize_text(self, input_text):\n",
        "        doc = self.nlp(input_text)\n",
        "        return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "    def remove_special_characters(self, input_text):\n",
        "        return re.sub(r'[^A-Za-z\\s]', ' ', input_text)\n",
        "\n",
        "    def tokenize_text(self, input_text):\n",
        "        return nltk.word_tokenize(input_text)\n",
        "\n",
        "    def preprocess(self, input_text, with_tokenize=False):\n",
        "        input_text = self.to_lowercase(input_text)\n",
        "        input_text = self.remove_punctuation(input_text)\n",
        "        input_text = self.remove_special_characters(input_text)\n",
        "        input_text = self.remove_whitespace(input_text)\n",
        "        input_text = self.remove_stopwords(input_text)\n",
        "        if with_tokenize:\n",
        "            input_text = self.tokenize_text(input_text)\n",
        "        return input_text\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ AnoVaText\n",
        "class AnoVaText:\n",
        "    def __init__(self, texts, labels, num_important_words):\n",
        "        if len(texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_words = self.extract_unique_words(texts)\n",
        "        self.word_counts = self.compute_word_counts(texts)\n",
        "\n",
        "    def extract_unique_words(self, texts):\n",
        "        words = []\n",
        "        for text in texts:\n",
        "            words.extend(text.split())\n",
        "        return list(set(words))\n",
        "\n",
        "    def compute_word_counts(self, texts):\n",
        "        word_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            word_counts.append(count)\n",
        "        return word_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_words = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_word_counts = [self.word_counts[i] for i in label_indices]\n",
        "            other_word_counts = [self.word_counts[i] for i in other_indices]\n",
        "\n",
        "            word_scores = {}\n",
        "            for word in self.unique_words:\n",
        "                label_word_frequencies = [count.get(word, 0) for count in label_word_counts]\n",
        "                other_word_frequencies = [count.get(word, 0) for count in other_word_counts]\n",
        "\n",
        "                if np.var(label_word_frequencies) > 0 and np.var(other_word_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_word_frequencies, other_word_frequencies)\n",
        "                    word_scores[word] = (f_stat, np.var(label_word_frequencies))\n",
        "                else:\n",
        "                    word_scores[word] = (0, np.var(label_word_frequencies))\n",
        "\n",
        "            sorted_words = sorted(word_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_words[label] = sorted_words[:self.num_important_words]\n",
        "\n",
        "        return important_words\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯ÛŒØªØ§Ø³Øª\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "print(\"length \", len(train_data))\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
        "txt_prs = TextPreprocessor()\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # Ø­Ø°Ù `and` Ùˆ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨ÛŒÙ† ØªÚ¯â€ŒÙ‡Ø§ Ùˆ Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(txt_prs.preprocess(text, with_tokenize=False))  # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
        "        # Ø´Ø±Ø· Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# Ø´Ù…Ø§Ø±Ø´ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡\n",
        "cause_effect_count = target_list.count(\"cause-effect\")\n",
        "others_count = target_list.count(\"others\")\n",
        "\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ cause-effect: {cause_effect_count}\")\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ others: {others_count}\")\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ø´ÛŒØ¡ Ø§Ø² TextAnalyzerANOVA\n",
        "analyzer = AnoVaText(text_list, target_list, 50)\n",
        "important_words = analyzer.analyze()\n",
        "\n",
        "# Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "with pd.ExcelWriter('important_words.xlsx') as writer:\n",
        "    for key, value in important_words.items():\n",
        "        df = pd.DataFrame([(word, score[0], score[1]) for word, score in value], columns=['Word', 'Score', 'Variance'])\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words have been written to the Excel file 'important_words.xlsx'.\")"
      ],
      "metadata": {
        "id": "797v43Tezvvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ AnoVaTextBigram\n",
        "class AnoVaTextBigram:\n",
        "    def __init__(self, bigram_texts, labels, num_important_words):\n",
        "        if len(bigram_texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = bigram_texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_bigrams = self.extract_unique_bigrams(bigram_texts)\n",
        "        self.bigram_counts = self.compute_bigram_counts(bigram_texts)\n",
        "\n",
        "    def extract_unique_bigrams(self, texts):\n",
        "        bigrams = []\n",
        "        for text in texts:\n",
        "            bigrams.extend(text.split())\n",
        "        return list(set(bigrams))\n",
        "\n",
        "    def compute_bigram_counts(self, texts):\n",
        "        bigram_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            bigram_counts.append(count)\n",
        "        return bigram_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_bigrams = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_bigram_counts = [self.bigram_counts[i] for i in label_indices]\n",
        "            other_bigram_counts = [self.bigram_counts[i] for i in other_indices]\n",
        "\n",
        "            bigram_scores = {}\n",
        "            for bigram in self.unique_bigrams:\n",
        "                label_bigram_frequencies = [count.get(bigram, 0) for count in label_bigram_counts]\n",
        "                other_bigram_frequencies = [count.get(bigram, 0) for count in other_bigram_counts]\n",
        "\n",
        "                if np.var(label_bigram_frequencies) > 0 and np.var(other_bigram_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_bigram_frequencies, other_bigram_frequencies)\n",
        "                    bigram_scores[bigram] = (f_stat, np.var(label_bigram_frequencies))\n",
        "                else:\n",
        "                    bigram_scores[bigram] = (0, np.var(label_bigram_frequencies))\n",
        "\n",
        "            sorted_bigrams = sorted(bigram_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_bigrams[label] = sorted_bigrams[:self.num_important_words]\n",
        "\n",
        "        return important_bigrams\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯ÛŒØªØ§Ø³Øª\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # Ø­Ø°Ù `and` Ùˆ Ú©Ù„Ù…Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨ÛŒÙ† ØªÚ¯â€ŒÙ‡Ø§ Ùˆ Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(preprocess_text(text))  # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†\n",
        "\n",
        "        # Ø´Ø±Ø· Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# ØªÙˆÙ„ÛŒØ¯ bigram Ù‡Ø§ Ø§Ø² Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
        "bigram_features = vectorizer.fit_transform(text_list)\n",
        "bigram_terms = vectorizer.get_feature_names_out()\n",
        "bigram_terms = [term.replace(' ', '_') for term in bigram_terms]\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø² bigram Ù‡Ø§\n",
        "bigram_data = [' '.join([bigram_terms[i] for i in row.nonzero()[1]]) for row in bigram_features]\n",
        "df = pd.DataFrame({'bigram_text': bigram_data, 'label': target_list})\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ø´ÛŒØ¡ Ø§Ø² TextAnalyzerANOVA Ø¨Ø§ bigram Ù‡Ø§\n",
        "analyzer = AnoVaTextBigram(df['bigram_text'], df['label'], 50)\n",
        "important_bigrams = analyzer.analyze()\n",
        "\n",
        "# Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "with pd.ExcelWriter('important_bigrams.xlsx') as writer:\n",
        "    for key, value in important_bigrams.items():\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…\n",
        "        df = pd.DataFrame([(bigram, score[0], score[1]) for bigram, score in value], columns=['Bigram', 'Score', 'Variance'])\n",
        "        # Ù†ÙˆØ´ØªÙ† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ù‡ ØµÙØ­Ù‡ Ø§Ú©Ø³Ù„\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important bigrams have been written to the Excel file 'important_bigrams.xlsx'.\")"
      ],
      "metadata": {
        "id": "Wdula3aiz5F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ `unigram` Ùˆ `bigram`\n",
        "combined_results = {}\n",
        "\n",
        "# Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§\n",
        "labels = ['cause-effect', 'others']\n",
        "\n",
        "for key in labels:\n",
        "    if key in important_words:\n",
        "        unigram_data = important_words[key]\n",
        "        unigram_df = pd.DataFrame(unigram_data, columns=['Term', 'Values'])\n",
        "        unigram_df[['Score', 'Variance']] = pd.DataFrame(unigram_df['Values'].tolist(), index=unigram_df.index)\n",
        "        unigram_df = unigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        unigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    if key in important_bigrams:\n",
        "        bigram_data = important_bigrams[key]\n",
        "        bigram_df = pd.DataFrame(bigram_data, columns=['Term', 'Values'])\n",
        "        bigram_df[['Score', 'Variance']] = pd.DataFrame(bigram_df['Values'].tolist(), index=bigram_df.index)\n",
        "        bigram_df = bigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        bigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    # ØªØ±Ú©ÛŒØ¨ Ø¯Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…\n",
        "    combined_df = pd.concat([unigram_df, bigram_df], ignore_index=True)\n",
        "\n",
        "    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ Score\n",
        "    combined_df = combined_df.sort_values(by='Score', ascending=False)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ\n",
        "    combined_results[key] = combined_df\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ø§ Ø¨Ø±Ú¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ Ù„ÛŒØ¨Ù„\n",
        "with pd.ExcelWriter('important_words_and_bigrams_combined.xlsx') as writer:\n",
        "  for key, df in combined_results.items():\n",
        "    df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words and bigrams have been written to the Excel file 'important_words_and_bigrams_combined.xlsx'.\")"
      ],
      "metadata": {
        "id": "t0g2y-0qz-g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "# Ú¯Ø§Ù… Û±: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# Ú¯Ø§Ù… Û²: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù… Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "cause_effect_df = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')\n",
        "important_terms = set(cause_effect_df['Term'].tolist())\n",
        "\n",
        "# Ú¯Ø§Ù… Û³: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "txt_prs = TextPreprocessor()\n",
        "\n",
        "def extract_entities(sentence):\n",
        "    try:\n",
        "        entity1 = re.search(r\"<e1>(.*?)</e1>\", sentence).group(1)\n",
        "        entity2 = re.search(r\"<e2>(.*?)</e2>\", sentence).group(1)\n",
        "        return entity1, entity2\n",
        "    except AttributeError:\n",
        "        return \"\", \"\"\n",
        "\n",
        "def get_terms_between_entities(sentence):\n",
        "    entity1, entity2 = extract_entities(sentence)\n",
        "    if entity1 and entity2:\n",
        "        terms_between_entities = re.search(r'<e1>.*?</e1>(.*?)<e2>.*?</e2>', sentence)\n",
        "        if terms_between_entities:\n",
        "            return terms_between_entities.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "train_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(train_data, train_data['relation'])))\n",
        "train_texts_labels = [(text, label) for text, label in train_texts_labels if text and label]  # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ\n",
        "train_texts, train_labels = zip(*train_texts_labels)\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª\n",
        "test_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(test_data, test_data['relation'])))\n",
        "test_texts_labels = [(text, label) for text, label in test_texts_labels if text and label]  # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ\n",
        "test_texts, test_labels = zip(*test_texts_labels)\n",
        "\n",
        "# Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "np.random.seed(42)  # ØªÙ†Ø¸ÛŒÙ… random_state Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
        "cause_effect_train = [(text, label) for text, label in train_texts_labels if label == 'cause-effect']\n",
        "others_train = [(text, label) for text, label in train_texts_labels if label == 'others']\n",
        "\n",
        "min_train_samples = min(len(cause_effect_train), len(others_train))\n",
        "balanced_train_data = cause_effect_train[:min_train_samples] + others_train[:min_train_samples]\n",
        "np.random.shuffle(balanced_train_data)  # Ø¨Ù‡â€ŒÙ‡Ù…â€ŒØ²Ø¯Ù† ØªØ±ØªÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
        "\n",
        "# Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª\n",
        "np.random.seed(42)  # ØªÙ†Ø¸ÛŒÙ… random_state Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª\n",
        "cause_effect_test = [(text, label) for text, label in test_texts_labels if label == 'cause-effect']\n",
        "others_test = [(text, label) for text, label in test_texts_labels if label == 'others']\n",
        "\n",
        "min_test_samples = min(len(cause_effect_test), len(others_test))\n",
        "balanced_test_data = cause_effect_test[:min_test_samples] + others_test[:min_test_samples]\n",
        "np.random.shuffle(balanced_test_data)  # Ø¨Ù‡â€ŒÙ‡Ù…â€ŒØ²Ø¯Ù† ØªØ±ØªÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
        "\n",
        "# Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
        "balanced_train_texts, balanced_train_labels = zip(*balanced_train_data)\n",
        "balanced_test_texts, balanced_test_labels = zip(*balanced_test_data)\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ DataFrame\n",
        "train_data_df = pd.DataFrame({\n",
        "    'Text': balanced_train_texts,\n",
        "    'Label': balanced_train_labels\n",
        "})\n",
        "\n",
        "test_data_df = pd.DataFrame({\n",
        "    'Text': balanced_test_texts,\n",
        "    'Label': balanced_test_labels\n",
        "})\n",
        "\n",
        "# ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± NaN\n",
        "train_data_df.dropna(subset=['Text', 'Label'], inplace=True)\n",
        "test_data_df.dropna(subset=['Text', 'Label'], inplace=True)\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ÛŒÚ© ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ø§ ØµÙØ­Ø§Øª Ù…Ø®ØªÙ„Ù\n",
        "with pd.ExcelWriter('balanced_data.xlsx') as writer:\n",
        "    train_data_df.to_excel(writer, sheet_name='Train Data', index=False)\n",
        "    test_data_df.to_excel(writer, sheet_name='Test Data', index=False)"
      ],
      "metadata": {
        "id": "0ZnQUuKH0BE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Model - First Try**"
      ],
      "metadata": {
        "id": "sfARqADbU4I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "# ØªØºÛŒÛŒØ± Ø¯Ø± Ø¨Ø®Ø´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª\n",
        "balanced_test_df = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')\n",
        "\n",
        "\n",
        "balanced_test_texts = balanced_test_df['Text'].tolist()\n",
        "balanced_test_labels = balanced_test_df['Label'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "important_terms = set(pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')['Term'].tolist())\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù… Ø¨Ù‡ ÛŒÚ© ÙˆÚ©ØªÙˆØ±\n",
        "vectorizer = CountVectorizer(vocabulary=important_terms, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(balanced_train_texts).toarray()\n",
        "X_test = vectorizer.transform(balanced_test_texts).toarray()\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Grid Search Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
        "param_grid = {\n",
        "    'logreg': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'rf': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'gb': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "}\n",
        "\n",
        "# Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Grid Search\n",
        "models = {\n",
        "    \"Logistic Regression\": (logreg, param_grid['logreg']),\n",
        "    \"Random Forest\": (rf, param_grid['rf']),\n",
        "    \"Gradient Boosting\": (gb, param_grid['gb']),\n",
        "}\n",
        "\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‡Ø± Ù…Ø¯Ù„ Ø¨Ø§ Grid Search\n",
        "best_models = {}\n",
        "for name, (model, params) in models.items():\n",
        "    print(f\"\\nPerforming Grid Search for {name}...\")\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=params,\n",
        "        cv=5,\n",
        "        scoring='f1_weighted',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "\n",
        "    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„\n",
        "    best_model = grid_search.best_estimator_\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\nBest parameters for {name}:\")\n",
        "    print(grid_search.best_params_)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ø§Ù†Ø³Ø§Ù…Ø¨Ù„ Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡\n",
        "# Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†Ø¬Ø§Ù… Grid Search Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', best_models['Logistic Regression']),\n",
        "        ('rf', best_models['Random Forest']),\n",
        "        ('gb', best_models['Gradient Boosting']),\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[1, 1.2, 1.2]  # ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§\n",
        ")\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ù†Ø³Ø§Ù…Ø¨Ù„\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø§Ù†Ø³Ø§Ù…Ø¨Ù„\n",
        "predictions = ensemble.predict(X_test)\n",
        "probabilities = ensemble.predict_proba(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions)\n",
        "\n",
        "# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "print(\"\\nEnsemble Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# ØªØ­Ù„ÛŒÙ„ False Positive Ù‡Ø§ Ø¨Ø§ LIME\n",
        "false_positives = [(text, true_label, pred_label) for text, true_label, pred_label in zip(balanced_test_texts, y_test, predictions) if true_label != pred_label and pred_label == 'cause-effect']\n",
        "\n",
        "lime_explainer = LimeTextExplainer(class_names=['cause-effect', 'others'])\n",
        "for i, (text, true_label, pred_label) in enumerate(false_positives[:5]):  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 5 Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´\n",
        "    print(f\"\\nFalse Positive Example {i+1}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {pred_label}\")\n",
        "    exp = lime_explainer.explain_instance(text, lambda x: ensemble.predict_proba(vectorizer.transform(x)), num_features=6)\n",
        "    exp.show_in_notebook(text=True)\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentence': balanced_test_texts,\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': predictions,\n",
        "    'Prediction Probability': probabilities.max(axis=1)\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter('improved_prediction_results.xlsx') as writer:\n",
        "    results_df.to_excel(writer, index=False)\n",
        "\n",
        "# ØªØ±Ø³ÛŒÙ… Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['cause-effect', 'others'],\n",
        "            yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Ensemble Model')\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zAVKKYXw0Gk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Model - second Try**"
      ],
      "metadata": {
        "id": "YxJTOI0ZVB4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ\n",
        "vectorizer = CountVectorizer(\n",
        "    vocabulary=important_terms,\n",
        "    ngram_range=(1, 2),\n",
        "    #sublinear_tf=True\n",
        ")\n",
        "X_train = vectorizer.fit_transform(balanced_train_texts).toarray()\n",
        "X_test = vectorizer.transform(balanced_test_texts).toarray()\n",
        "\n",
        "# Ù…Ù‚ÛŒØ§Ø³â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù„Ø§Ø²Ù… Ø¯Ø§Ø±Ù†\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "models_to_test = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "results = []\n",
        "for name, model in models_to_test.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    # Ø§Ø¹Ù…Ø§Ù„ Ø§Ø³Ú©ÛŒÙ„ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†\n",
        "    if \"Scaled\" in name:\n",
        "        X_tr = X_train_scaled\n",
        "        X_te = X_test_scaled\n",
        "    else:\n",
        "        X_tr = X_train\n",
        "        X_te = X_test\n",
        "\n",
        "    model.fit(X_tr, y_train)\n",
        "    preds = model.predict(X_te)\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    prec = precision_score(y_test, preds, average='weighted')\n",
        "    rec = recall_score(y_test, preds, average='weighted')\n",
        "    f1 = f1_score(y_test, preds, average='weighted')\n",
        "\n",
        "    results.append((name, acc, prec, rec, f1))\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "    # confusion matrix\n",
        "    conf = confusion_matrix(y_test, preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['cause-effect', 'others'],\n",
        "                yticklabels=['cause-effect', 'others'])\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ø¬Ø¯ÙˆÙ„ Ù†ØªØ§ÛŒØ¬\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
        "print(\"\\nğŸ”¬ Model Comparison with important_terms Vocabulary:\")\n",
        "print(results_df)\n",
        "\n",
        "# ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±\n",
        "results_df.set_index(\"Model\").plot(kind='bar', figsize=(12, 6), ylim=(0.5, 1.0), colormap='tab10', grid=True)\n",
        "plt.title(\"ğŸ“Š Performance Comparison (Preserving important_terms)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OADLIHPnAILO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ensemble\n",
        "ensemble_models = [\n",
        "    ('lr', LogisticRegression(random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
        "]\n",
        "\n",
        "# Ø³Ø§Ø®Øª VotingClassifier Ø¨Ø§ Ù†Ø³Ø®Ù‡ scaleâ€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ø§Ø³\n",
        "# Ù†Ø³Ø®Ù‡ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ Ù…Ù‚ÛŒØ§Ø³â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=ensemble_models,\n",
        "    voting='soft',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ù†Ø³Ø®Ù‡ Ù…ØªÙ†Ø§Ø³Ø¨\n",
        "ensemble.fit(X_train_scaled, y_train)  # Ú†ÙˆÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ† Ú©Ù‡ Ø§Ø³Ú©ÛŒÙ„ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†\n",
        "\n",
        "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "ensemble_preds = ensemble.predict(X_test_scaled)\n",
        "ensemble_probs = ensemble.predict_proba(X_test_scaled)\n",
        "acc = accuracy_score(y_test, ensemble_preds)\n",
        "prec = precision_score(y_test, ensemble_preds, average='weighted')\n",
        "rec = recall_score(y_test, ensemble_preds, average='weighted')\n",
        "f1 = f1_score(y_test, ensemble_preds, average='weighted')\n",
        "\n",
        "# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬\n",
        "print(\"\\nğŸ”— Voting Ensemble Performance (Preserving important_terms):\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall: {rec:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, ensemble_preds))"
      ],
      "metadata": {
        "id": "lgRHA9h7AOmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ (Base learners)\n",
        "base_estimators = [\n",
        "    ('lr', LogisticRegression(random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
        "]\n",
        "\n",
        "# Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ: Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡\n",
        "meta_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=300, random_state=42)\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Stacking Ø¨Ø§ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ\n",
        "stacking_nn = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=meta_model,\n",
        "    passthrough=True,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡\n",
        "stacking_nn.fit(X_train, y_train)\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "nn_preds = stacking_nn.predict(X_test)\n",
        "nn_probs = stacking_nn.predict_proba(X_test)\n",
        "\n",
        "# Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§\n",
        "print(\"\\nğŸ§  Stacking with Neural Network Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, nn_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, nn_preds))"
      ],
      "metadata": {
        "id": "fDteAlm-ASaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Model**"
      ],
      "metadata": {
        "id": "1mbW3d16VI_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "balanced_test_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Text'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ\n",
        "unique_classes = np.unique(y_train)\n",
        "random_predictions = np.array([random.choice(unique_classes) for _ in range(len(y_test))])\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
        "accuracy = accuracy_score(y_test, random_predictions)\n",
        "precision = precision_score(y_test, random_predictions, average='weighted')\n",
        "recall = recall_score(y_test, random_predictions, average='weighted')\n",
        "f1 = f1_score(y_test, random_predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, random_predictions)\n",
        "class_report = classification_report(y_test, random_predictions)\n",
        "\n",
        "# Ú†Ø§Ù¾ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
        "print(\"=== Random Baseline Results ===\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentence': balanced_test_texts,\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': random_predictions,\n",
        "    'Prediction Probability': 0.5  # Ø§Ø­ØªÙ…Ø§Ù„ 50% Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªØµØ§Ø¯ÙÛŒ\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter('random_baseline_results.xlsx') as writer:\n",
        "    results_df.to_excel(writer, index=False)\n",
        "\n",
        "# ØªØ±Ø³ÛŒÙ… Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['cause-effect', 'others'],\n",
        "            yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Random Baseline')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lgGielsLHF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule-Based Model**"
      ],
      "metadata": {
        "id": "Y0GyJrE7VM8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "balanced_test_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Text'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "important_terms = set(pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')['Term'].tolist())\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# ==============================================\n",
        "# 1. Ù…Ø¯Ù„ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ (Rule-Based)\n",
        "# ==============================================\n",
        "print(\"\\n=== Rule-Based Baseline ===\")\n",
        "rule_based_predictions = [\n",
        "    \"cause-effect\" if any(term in text.lower() for term in important_terms)\n",
        "    else \"others\"\n",
        "    for text in balanced_test_texts\n",
        "]\n",
        "\n",
        "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
        "accuracy = accuracy_score(y_test, rule_based_predictions)\n",
        "precision = precision_score(y_test, rule_based_predictions, average='weighted')\n",
        "recall = recall_score(y_test, rule_based_predictions, average='weighted')\n",
        "f1 = f1_score(y_test, rule_based_predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, rule_based_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "# ØªØ±Ø³ÛŒÙ… Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['cause-effect', 'others'], yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wVN070JkHITo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph**"
      ],
      "metadata": {
        "id": "tJBXTC93VRPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import inflect\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = 'all_sentences_with_original.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Extract the entities and labels\n",
        "df['Entity1'] = df['e1']\n",
        "df['Entity2'] = df['e2']\n",
        "\n",
        "# Filter out rows where Entity1 or Entity2 is empty\n",
        "df = df.dropna(subset=['Entity1', 'Entity2'])\n",
        "\n",
        "# Initialize inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# # Normalize entities to singular form\n",
        "def normalize_entity(entity):\n",
        "    entity = str(entity).strip()  # ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ùˆ Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
        "    words = entity.split()  # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª\n",
        "    if len(words) > 1:  # Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø´Ø¯ØŒ ØªØºÛŒÛŒØ± Ù†Ø¯Ù‡ÛŒÙ…\n",
        "        return entity\n",
        "    singular = p.singular_noun(entity)\n",
        "    return singular if isinstance(singular, str) else entity  # ØªØºÛŒÛŒØ± ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù† Ù…Ù‚Ø¯Ø§Ø±\n",
        "\n",
        "# Apply normalization safely\n",
        "df['Entity1'] = df['Entity1'].apply(normalize_entity)\n",
        "df['Entity2'] = df['Entity2'].apply(normalize_entity)\n",
        "\n",
        "# Select relevant columns for Cytoscape and rename columns to source and target\n",
        "cytoscape_df = df[['Entity1', 'Entity2', 'Predicted Label']]\n",
        "cytoscape_df.columns = ['source', 'target', 'label']\n",
        "\n",
        "# Filter out rows where label is 'others'\n",
        "cytoscape_df = cytoscape_df[cytoscape_df['label'] != 'others']\n",
        "\n",
        "# Save the data to a CSV file\n",
        "cytoscape_df.to_csv('cytoscape_data_causal_V040520.csv', index=False)"
      ],
      "metadata": {
        "id": "fz3bQeHeATPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**False Positives and False Negatives**"
      ],
      "metadata": {
        "id": "qIfnQDRVVUmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„\n",
        "results_df = pd.read_excel('all_sentences_with_original.xlsx')\n",
        "\n",
        "# ØªØ¹Ø±ÛŒÙ False Positive Ùˆ False Negative\n",
        "false_positives = results_df[\n",
        "    (results_df['Predicted Label'] == 'cause-effect') &\n",
        "    (results_df['True Label'] == 'others')\n",
        "]\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…ÙˆØ§Ø±Ø¯ FP Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {len(false_positives)}\")\n",
        "false_negatives = results_df[\n",
        "    (results_df['True Label'] == 'cause-effect') &\n",
        "    (results_df['Predicted Label'] == 'others')\n",
        "]\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ØªØ¨â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡\n",
        "with pd.ExcelWriter('error_analysis.xlsx') as writer:\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ØªØ¨ Ø§ÙˆÙ„\n",
        "    results_df.to_excel(writer, sheet_name='All Data', index=False)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ False Positive Ø¯Ø± ØªØ¨ Ø¯ÙˆÙ…\n",
        "    false_positives.to_excel(writer, sheet_name='False Positives', index=False)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ False Negative Ø¯Ø± ØªØ¨ Ø³ÙˆÙ…\n",
        "    false_negatives.to_excel(writer, sheet_name='False Negatives', index=False)\n",
        "\n",
        "print(\"âœ… ÙØ§ÛŒÙ„ error_analysis.xlsx Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯!\")\n",
        "print(f\"ğŸ” ØªØ¹Ø¯Ø§Ø¯ False Positives: {len(false_positives)}\")\n",
        "print(f\"ğŸ” ØªØ¹Ø¯Ø§Ø¯ False Negatives: {len(false_negatives)}\")"
      ],
      "metadata": {
        "id": "IHAnPVcFAg7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}