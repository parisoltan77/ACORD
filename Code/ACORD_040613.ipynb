{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sANeJJKayaQH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets scikit-learn lime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "oRDyUaoJznYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "CstTVPgszrAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "!pip install fsspec==2023.9.2"
      ],
      "metadata": {
        "id": "l_Yyajba0Qef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = { 0: 'Cause-Effect-e2-e1', 1: 'Cause-Effect-e1-e2', 2: 'Component-Whole', 3: 'Component-Whole', 4: 'Content-Container', 5: 'Content-Container', 6: 'Entity-Destination', 7: 'Entity-Destination', 8: 'Entity-Origin', 9: 'Entity-Origin', 10: 'Instrument-Agency', 11: 'Instrument-Agency', 12: 'Member-Collection', 13: 'Member-Collection', 14: 'Message-Topic', 15: 'Message-Topic', 16: 'Product-Producer', 17: 'Product-Producer', 18: 'Other' }"
      ],
      "metadata": {
        "id": "IXFEQHRyzulc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA Method**"
      ],
      "metadata": {
        "id": "a0XY_X1DzwKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # دانلود منابع punkt_tab برای رفع خطا\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def to_lowercase(self, input_text):\n",
        "        return input_text.lower()\n",
        "\n",
        "    def remove_punctuation(self, input_text):\n",
        "        return input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    def remove_whitespace(self, input_text):\n",
        "        return ' '.join(input_text.split())\n",
        "\n",
        "    def remove_stopwords(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(word for word in words if word not in self.stop_words)\n",
        "\n",
        "    def stem_text(self, input_text):\n",
        "        words = nltk.word_tokenize(input_text)\n",
        "        return ' '.join(self.stemmer.stem(word) for word in words)\n",
        "\n",
        "    def lemmatize_text(self, input_text):\n",
        "        doc = self.nlp(input_text)\n",
        "        return ' '.join(token.lemma_ for token in doc)\n",
        "\n",
        "    def remove_special_characters(self, input_text):\n",
        "        return re.sub(r'[^A-Za-z\\s]', ' ', input_text)\n",
        "\n",
        "    def tokenize_text(self, input_text):\n",
        "        return nltk.word_tokenize(input_text)\n",
        "\n",
        "    def preprocess(self, input_text, with_tokenize=False):\n",
        "        input_text = self.to_lowercase(input_text)\n",
        "        input_text = self.remove_punctuation(input_text)\n",
        "        input_text = self.remove_special_characters(input_text)\n",
        "        input_text = self.remove_whitespace(input_text)\n",
        "        input_text = self.remove_stopwords(input_text)\n",
        "        if with_tokenize:\n",
        "            input_text = self.tokenize_text(input_text)\n",
        "        return input_text\n",
        "\n",
        "# تعریف کلاس AnoVaText\n",
        "class AnoVaText:\n",
        "    def __init__(self, texts, labels, num_important_words):\n",
        "        if len(texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_words = self.extract_unique_words(texts)\n",
        "        self.word_counts = self.compute_word_counts(texts)\n",
        "\n",
        "    def extract_unique_words(self, texts):\n",
        "        words = []\n",
        "        for text in texts:\n",
        "            words.extend(text.split())\n",
        "        return list(set(words))\n",
        "\n",
        "    def compute_word_counts(self, texts):\n",
        "        word_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            word_counts.append(count)\n",
        "        return word_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_words = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_word_counts = [self.word_counts[i] for i in label_indices]\n",
        "            other_word_counts = [self.word_counts[i] for i in other_indices]\n",
        "\n",
        "            word_scores = {}\n",
        "            for word in self.unique_words:\n",
        "                label_word_frequencies = [count.get(word, 0) for count in label_word_counts]\n",
        "                other_word_frequencies = [count.get(word, 0) for count in other_word_counts]\n",
        "\n",
        "                if np.var(label_word_frequencies) > 0 and np.var(other_word_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_word_frequencies, other_word_frequencies)\n",
        "                    word_scores[word] = (f_stat, np.var(label_word_frequencies))\n",
        "                else:\n",
        "                    word_scores[word] = (0, np.var(label_word_frequencies))\n",
        "\n",
        "            sorted_words = sorted(word_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_words[label] = sorted_words[:self.num_important_words]\n",
        "\n",
        "        return important_words\n",
        "\n",
        "# بارگیری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "print(\"length \", len(train_data))\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "txt_prs = TextPreprocessor()\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # حذف `and` و کلمه بعد از `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # استخراج متن بین تگ‌ها و حذف فاصله‌های اضافی\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(txt_prs.preprocess(text, with_tokenize=False))  # پیش‌پردازش متن\n",
        "        # شرط برچسب‌ها\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# شمارش نمونه‌های هر دسته\n",
        "cause_effect_count = target_list.count(\"cause-effect\")\n",
        "others_count = target_list.count(\"others\")\n",
        "\n",
        "print(f\"تعداد نمونه‌های cause-effect: {cause_effect_count}\")\n",
        "print(f\"تعداد نمونه‌های others: {others_count}\")\n",
        "\n",
        "# ایجاد یک شیء از TextAnalyzerANOVA\n",
        "analyzer = AnoVaText(text_list, target_list, 50)\n",
        "important_words = analyzer.analyze()\n",
        "\n",
        "# خروجی به فایل اکسل\n",
        "with pd.ExcelWriter('important_words.xlsx') as writer:\n",
        "    for key, value in important_words.items():\n",
        "        df = pd.DataFrame([(word, score[0], score[1]) for word, score in value], columns=['Word', 'Score', 'Variance'])\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words have been written to the Excel file 'important_words.xlsx'.\")"
      ],
      "metadata": {
        "id": "797v43Tezvvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from scipy.stats import f_oneway\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# تعریف کلاس AnoVaTextBigram\n",
        "class AnoVaTextBigram:\n",
        "    def __init__(self, bigram_texts, labels, num_important_words):\n",
        "        if len(bigram_texts) != len(labels):\n",
        "            raise ValueError(\"The number of texts and labels must be the same.\")\n",
        "        self.texts = bigram_texts\n",
        "        self.labels = labels\n",
        "        self.num_important_words = num_important_words\n",
        "        self.unique_bigrams = self.extract_unique_bigrams(bigram_texts)\n",
        "        self.bigram_counts = self.compute_bigram_counts(bigram_texts)\n",
        "\n",
        "    def extract_unique_bigrams(self, texts):\n",
        "        bigrams = []\n",
        "        for text in texts:\n",
        "            bigrams.extend(text.split())\n",
        "        return list(set(bigrams))\n",
        "\n",
        "    def compute_bigram_counts(self, texts):\n",
        "        bigram_counts = []\n",
        "        for text in texts:\n",
        "            count = Counter(text.split())\n",
        "            bigram_counts.append(count)\n",
        "        return bigram_counts\n",
        "\n",
        "    def analyze(self):\n",
        "        label_set = set(self.labels)\n",
        "        important_bigrams = {}\n",
        "        for label in label_set:\n",
        "            label_indices = [i for i, lbl in enumerate(self.labels) if lbl == label]\n",
        "            other_indices = [i for i, lbl in enumerate(self.labels) if lbl != label]\n",
        "            label_bigram_counts = [self.bigram_counts[i] for i in label_indices]\n",
        "            other_bigram_counts = [self.bigram_counts[i] for i in other_indices]\n",
        "\n",
        "            bigram_scores = {}\n",
        "            for bigram in self.unique_bigrams:\n",
        "                label_bigram_frequencies = [count.get(bigram, 0) for count in label_bigram_counts]\n",
        "                other_bigram_frequencies = [count.get(bigram, 0) for count in other_bigram_counts]\n",
        "\n",
        "                if np.var(label_bigram_frequencies) > 0 and np.var(other_bigram_frequencies) > 0:\n",
        "                    f_stat, p_value = f_oneway(label_bigram_frequencies, other_bigram_frequencies)\n",
        "                    bigram_scores[bigram] = (f_stat, np.var(label_bigram_frequencies))\n",
        "                else:\n",
        "                    bigram_scores[bigram] = (0, np.var(label_bigram_frequencies))\n",
        "\n",
        "            sorted_bigrams = sorted(bigram_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "            important_bigrams[label] = sorted_bigrams[:self.num_important_words]\n",
        "\n",
        "        return important_bigrams\n",
        "\n",
        "# بارگیری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "print(ds)\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# پیش‌پردازش متن\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "text_list, target_list = [], []\n",
        "for row in train_data:\n",
        "    sentence = row[\"sentence\"]\n",
        "\n",
        "    # حذف `and` و کلمه بعد از `</e1>`\n",
        "    sentence = re.sub(r'</e1>\\s*and\\s*\\w+', '</e1>', sentence)\n",
        "\n",
        "    pattern = r\"<e1>.*?</e1>(.*?)<e2>.*?</e2>\"\n",
        "\n",
        "    match = re.search(pattern, sentence)\n",
        "    if match:\n",
        "        text = match.group(1).strip()  # استخراج متن بین تگ‌ها و حذف فاصله‌های اضافی\n",
        "        label = row[\"relation\"]\n",
        "        text_list.append(preprocess_text(text))  # پیش‌پردازش متن\n",
        "\n",
        "        # شرط برچسب‌ها\n",
        "        if label == 0 or label == 1:\n",
        "            target_list.append(\"cause-effect\")\n",
        "        else:\n",
        "            target_list.append(\"others\")\n",
        "\n",
        "# تولید bigram ها از رشته‌ها\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', analyzer='word')\n",
        "bigram_features = vectorizer.fit_transform(text_list)\n",
        "bigram_terms = vectorizer.get_feature_names_out()\n",
        "bigram_terms = [term.replace(' ', '_') for term in bigram_terms]\n",
        "\n",
        "# ساخت دیتافریم از bigram ها\n",
        "bigram_data = [' '.join([bigram_terms[i] for i in row.nonzero()[1]]) for row in bigram_features]\n",
        "df = pd.DataFrame({'bigram_text': bigram_data, 'label': target_list})\n",
        "\n",
        "# ایجاد یک شیء از TextAnalyzerANOVA با bigram ها\n",
        "analyzer = AnoVaTextBigram(df['bigram_text'], df['label'], 50)\n",
        "important_bigrams = analyzer.analyze()\n",
        "\n",
        "# خروجی به فایل اکسل\n",
        "with pd.ExcelWriter('important_bigrams.xlsx') as writer:\n",
        "    for key, value in important_bigrams.items():\n",
        "        # تبدیل نتیجه به دیتافریم\n",
        "        df = pd.DataFrame([(bigram, score[0], score[1]) for bigram, score in value], columns=['Bigram', 'Score', 'Variance'])\n",
        "        # نوشتن دیتافریم به صفحه اکسل\n",
        "        df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important bigrams have been written to the Excel file 'important_bigrams.xlsx'.\")"
      ],
      "metadata": {
        "id": "Wdula3aiz5F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ترکیب نتایج `unigram` و `bigram`\n",
        "combined_results = {}\n",
        "\n",
        "# لیبل‌ها\n",
        "labels = ['cause-effect', 'others']\n",
        "\n",
        "for key in labels:\n",
        "    if key in important_words:\n",
        "        unigram_data = important_words[key]\n",
        "        unigram_df = pd.DataFrame(unigram_data, columns=['Term', 'Values'])\n",
        "        unigram_df[['Score', 'Variance']] = pd.DataFrame(unigram_df['Values'].tolist(), index=unigram_df.index)\n",
        "        unigram_df = unigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        unigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    if key in important_bigrams:\n",
        "        bigram_data = important_bigrams[key]\n",
        "        bigram_df = pd.DataFrame(bigram_data, columns=['Term', 'Values'])\n",
        "        bigram_df[['Score', 'Variance']] = pd.DataFrame(bigram_df['Values'].tolist(), index=bigram_df.index)\n",
        "        bigram_df = bigram_df.drop(columns=['Values'])\n",
        "    else:\n",
        "        bigram_df = pd.DataFrame(columns=['Term', 'Score', 'Variance'])\n",
        "\n",
        "    # ترکیب دو دیتافریم\n",
        "    combined_df = pd.concat([unigram_df, bigram_df], ignore_index=True)\n",
        "\n",
        "    # مرتب‌سازی نتایج بر اساس Score\n",
        "    combined_df = combined_df.sort_values(by='Score', ascending=False)\n",
        "\n",
        "    # ذخیره نتایج ترکیبی در دیکشنری\n",
        "    combined_results[key] = combined_df\n",
        "\n",
        "# ذخیره نتایج در فایل اکسل با برگه‌های جداگانه برای هر کلاس لیبل\n",
        "with pd.ExcelWriter('important_words_and_bigrams_combined.xlsx') as writer:\n",
        "  for key, df in combined_results.items():\n",
        "    df.to_excel(writer, sheet_name=key, index=False)\n",
        "\n",
        "print(\"Important words and bigrams have been written to the Excel file 'important_words_and_bigrams_combined.xlsx'.\")"
      ],
      "metadata": {
        "id": "t0g2y-0qz-g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "# گام ۱: بارگذاری دیتاست\n",
        "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
        "train_data, test_data = ds[\"train\"], ds[\"test\"]\n",
        "\n",
        "# گام ۲: استخراج کلمات مهم از فایل اکسل\n",
        "cause_effect_df = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')\n",
        "important_terms = set(cause_effect_df['Term'].tolist())\n",
        "\n",
        "# گام ۳: آماده‌سازی داده‌ها\n",
        "txt_prs = TextPreprocessor()\n",
        "\n",
        "def extract_entities(sentence):\n",
        "    try:\n",
        "        entity1 = re.search(r\"<e1>(.*?)</e1>\", sentence).group(1)\n",
        "        entity2 = re.search(r\"<e2>(.*?)</e2>\", sentence).group(1)\n",
        "        return entity1, entity2\n",
        "    except AttributeError:\n",
        "        return \"\", \"\"\n",
        "\n",
        "def get_terms_between_entities(sentence):\n",
        "    entity1, entity2 = extract_entities(sentence)\n",
        "    if entity1 and entity2:\n",
        "        terms_between_entities = re.search(r'<e1>.*?</e1>(.*?)<e2>.*?</e2>', sentence)\n",
        "        if terms_between_entities:\n",
        "            return terms_between_entities.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "# ایجاد لیست‌های یکتا برای داده‌های آموزشی\n",
        "train_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(train_data, train_data['relation'])))\n",
        "train_texts_labels = [(text, label) for text, label in train_texts_labels if text and label]  # حذف سطرهای خالی\n",
        "train_texts, train_labels = zip(*train_texts_labels)\n",
        "\n",
        "# ایجاد لیست‌های یکتا برای داده‌های تست\n",
        "test_texts_labels = list(set((get_terms_between_entities(row['sentence']), 'cause-effect' if label == 0 or label == 1 else 'others') for row, label in zip(test_data, test_data['relation'])))\n",
        "test_texts_labels = [(text, label) for text, label in test_texts_labels if text and label]  # حذف سطرهای خالی\n",
        "test_texts, test_labels = zip(*test_texts_labels)\n",
        "\n",
        "# متعادل‌سازی داده‌های آموزشی\n",
        "np.random.seed(42)  # تنظیم random_state برای داده‌های آموزشی\n",
        "cause_effect_train = [(text, label) for text, label in train_texts_labels if label == 'cause-effect']\n",
        "others_train = [(text, label) for text, label in train_texts_labels if label == 'others']\n",
        "\n",
        "min_train_samples = min(len(cause_effect_train), len(others_train))\n",
        "balanced_train_data = cause_effect_train[:min_train_samples] + others_train[:min_train_samples]\n",
        "np.random.shuffle(balanced_train_data)  # به‌هم‌زدن ترتیب داده‌ها پس از متعادل‌سازی\n",
        "\n",
        "# متعادل‌سازی داده‌های تست\n",
        "np.random.seed(42)  # تنظیم random_state برای داده‌های تست\n",
        "cause_effect_test = [(text, label) for text, label in test_texts_labels if label == 'cause-effect']\n",
        "others_test = [(text, label) for text, label in test_texts_labels if label == 'others']\n",
        "\n",
        "min_test_samples = min(len(cause_effect_test), len(others_test))\n",
        "balanced_test_data = cause_effect_test[:min_test_samples] + others_test[:min_test_samples]\n",
        "np.random.shuffle(balanced_test_data)  # به‌هم‌زدن ترتیب داده‌ها پس از متعادل‌سازی\n",
        "\n",
        "# جدا کردن متن‌ها و برچسب‌ها\n",
        "balanced_train_texts, balanced_train_labels = zip(*balanced_train_data)\n",
        "balanced_test_texts, balanced_test_labels = zip(*balanced_test_data)\n",
        "\n",
        "# تبدیل داده‌ها به DataFrame\n",
        "train_data_df = pd.DataFrame({\n",
        "    'Text': balanced_train_texts,\n",
        "    'Label': balanced_train_labels\n",
        "})\n",
        "\n",
        "test_data_df = pd.DataFrame({\n",
        "    'Text': balanced_test_texts,\n",
        "    'Label': balanced_test_labels\n",
        "})\n",
        "\n",
        "# تمیز کردن داده‌ها و حذف مقادیر NaN\n",
        "train_data_df.dropna(subset=['Text', 'Label'], inplace=True)\n",
        "test_data_df.dropna(subset=['Text', 'Label'], inplace=True)\n",
        "\n",
        "# ذخیره داده‌ها به یک فایل اکسل با صفحات مختلف\n",
        "with pd.ExcelWriter('balanced_data.xlsx') as writer:\n",
        "    train_data_df.to_excel(writer, sheet_name='Train Data', index=False)\n",
        "    test_data_df.to_excel(writer, sheet_name='Test Data', index=False)"
      ],
      "metadata": {
        "id": "0ZnQUuKH0BE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Model - First Try**"
      ],
      "metadata": {
        "id": "sfARqADbU4I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "\n",
        "# بارگذاری داده‌ها از فایل اکسل\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "# تغییر در بخش بارگذاری و فیلتر کردن داده‌های تست\n",
        "balanced_test_df = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')\n",
        "\n",
        "\n",
        "balanced_test_texts = balanced_test_df['Text'].tolist()\n",
        "balanced_test_labels = balanced_test_df['Label'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "important_terms = set(pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')['Term'].tolist())\n",
        "\n",
        "# تبدیل کلمات مهم به یک وکتور\n",
        "vectorizer = CountVectorizer(vocabulary=important_terms, ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(balanced_train_texts).toarray()\n",
        "X_test = vectorizer.transform(balanced_test_texts).toarray()\n",
        "\n",
        "# تبدیل برچسب‌ها به آرایه numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# تعریف مدل‌ها با پارامترهای پیش‌فرض\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# تعریف پارامترهای Grid Search برای هر مدل\n",
        "param_grid = {\n",
        "    'logreg': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'rf': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'gb': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "}\n",
        "\n",
        "# لیست مدل‌ها برای Grid Search\n",
        "models = {\n",
        "    \"Logistic Regression\": (logreg, param_grid['logreg']),\n",
        "    \"Random Forest\": (rf, param_grid['rf']),\n",
        "    \"Gradient Boosting\": (gb, param_grid['gb']),\n",
        "}\n",
        "\n",
        "\n",
        "# آموزش و ارزیابی هر مدل با Grid Search\n",
        "best_models = {}\n",
        "for name, (model, params) in models.items():\n",
        "    print(f\"\\nPerforming Grid Search for {name}...\")\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=params,\n",
        "        cv=5,\n",
        "        scoring='f1_weighted',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # ذخیره بهترین مدل\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "\n",
        "    # ارزیابی بهترین مدل\n",
        "    best_model = grid_search.best_estimator_\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "    print(f\"\\nBest parameters for {name}:\")\n",
        "    print(grid_search.best_params_)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# ایجاد مدل انسامبل با بهترین مدل‌های یافت شده\n",
        "# بعد از انجام Grid Search برای همه مدل‌ها\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', best_models['Logistic Regression']),\n",
        "        ('rf', best_models['Random Forest']),\n",
        "        ('gb', best_models['Gradient Boosting']),\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[1, 1.2, 1.2]  # تنظیم وزن‌ها\n",
        ")\n",
        "\n",
        "# آموزش مدل انسامبل\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# پیش‌بینی و ارزیابی مدل انسامبل\n",
        "predictions = ensemble.predict(X_test)\n",
        "probabilities = ensemble.predict_proba(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions)\n",
        "\n",
        "# چاپ نتایج ارزیابی\n",
        "print(\"\\nEnsemble Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# تحلیل False Positive ها با LIME\n",
        "false_positives = [(text, true_label, pred_label) for text, true_label, pred_label in zip(balanced_test_texts, y_test, predictions) if true_label != pred_label and pred_label == 'cause-effect']\n",
        "\n",
        "lime_explainer = LimeTextExplainer(class_names=['cause-effect', 'others'])\n",
        "for i, (text, true_label, pred_label) in enumerate(false_positives[:5]):  # محدود کردن به 5 نمونه برای نمایش\n",
        "    print(f\"\\nFalse Positive Example {i+1}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"True Label: {true_label}, Predicted Label: {pred_label}\")\n",
        "    exp = lime_explainer.explain_instance(text, lambda x: ensemble.predict_proba(vectorizer.transform(x)), num_features=6)\n",
        "    exp.show_in_notebook(text=True)\n",
        "\n",
        "# ذخیره نتایج پیش‌بینی شده به فایل اکسل\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentence': balanced_test_texts,\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': predictions,\n",
        "    'Prediction Probability': probabilities.max(axis=1)\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter('improved_prediction_results.xlsx') as writer:\n",
        "    results_df.to_excel(writer, index=False)\n",
        "\n",
        "# ترسیم ماتریس درهم‌ریختگی\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['cause-effect', 'others'],\n",
        "            yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Ensemble Model')\n",
        "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zAVKKYXw0Gk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Model - second Try**"
      ],
      "metadata": {
        "id": "YxJTOI0ZVB4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# بردارسازی پیشرفته با کلمات کلیدی\n",
        "vectorizer = CountVectorizer(\n",
        "    vocabulary=important_terms,\n",
        "    ngram_range=(1, 2),\n",
        "    #sublinear_tf=True\n",
        ")\n",
        "X_train = vectorizer.fit_transform(balanced_train_texts).toarray()\n",
        "X_test = vectorizer.transform(balanced_test_texts).toarray()\n",
        "\n",
        "# مقیاس‌گذاری برای مدل‌هایی که لازم دارن\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# مدل‌ها\n",
        "models_to_test = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "# ارزیابی\n",
        "results = []\n",
        "for name, model in models_to_test.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    # اعمال اسکیل فقط برای مدل‌هایی که نیاز دارن\n",
        "    if \"Scaled\" in name:\n",
        "        X_tr = X_train_scaled\n",
        "        X_te = X_test_scaled\n",
        "    else:\n",
        "        X_tr = X_train\n",
        "        X_te = X_test\n",
        "\n",
        "    model.fit(X_tr, y_train)\n",
        "    preds = model.predict(X_te)\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    prec = precision_score(y_test, preds, average='weighted')\n",
        "    rec = recall_score(y_test, preds, average='weighted')\n",
        "    f1 = f1_score(y_test, preds, average='weighted')\n",
        "\n",
        "    results.append((name, acc, prec, rec, f1))\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "    # confusion matrix\n",
        "    conf = confusion_matrix(y_test, preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['cause-effect', 'others'],\n",
        "                yticklabels=['cause-effect', 'others'])\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# جدول نتایج\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
        "print(\"\\n🔬 Model Comparison with important_terms Vocabulary:\")\n",
        "print(results_df)\n",
        "\n",
        "# ترسیم نمودار\n",
        "results_df.set_index(\"Model\").plot(kind='bar', figsize=(12, 6), ylim=(0.5, 1.0), colormap='tab10', grid=True)\n",
        "plt.title(\"📊 Performance Comparison (Preserving important_terms)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OADLIHPnAILO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# مدل‌های پایه برای Ensemble\n",
        "ensemble_models = [\n",
        "    ('lr', LogisticRegression(random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
        "]\n",
        "\n",
        "# ساخت VotingClassifier با نسخه scale‌شده برای مدل‌های حساس\n",
        "# نسخه خاص برای مقیاس‌گذاری\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=ensemble_models,\n",
        "    voting='soft',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# آموزش روی نسخه متناسب\n",
        "ensemble.fit(X_train_scaled, y_train)  # چون مدل‌هایی هستن که اسکیل نیاز دارن\n",
        "\n",
        "# پیش‌بینی و ارزیابی\n",
        "ensemble_preds = ensemble.predict(X_test_scaled)\n",
        "ensemble_probs = ensemble.predict_proba(X_test_scaled)\n",
        "acc = accuracy_score(y_test, ensemble_preds)\n",
        "prec = precision_score(y_test, ensemble_preds, average='weighted')\n",
        "rec = recall_score(y_test, ensemble_preds, average='weighted')\n",
        "f1 = f1_score(y_test, ensemble_preds, average='weighted')\n",
        "\n",
        "# چاپ نتایج\n",
        "print(\"\\n🔗 Voting Ensemble Performance (Preserving important_terms):\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall: {rec:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, ensemble_preds))"
      ],
      "metadata": {
        "id": "lgRHA9h7AOmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# مدل‌های پایه (Base learners)\n",
        "base_estimators = [\n",
        "    ('lr', LogisticRegression(random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
        "]\n",
        "\n",
        "# مدل نهایی: شبکه عصبی چندلایه\n",
        "meta_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=300, random_state=42)\n",
        "\n",
        "# ساخت مدل Stacking با شبکه عصبی\n",
        "stacking_nn = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=meta_model,\n",
        "    passthrough=True,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# آموزش روی داده\n",
        "stacking_nn.fit(X_train, y_train)\n",
        "\n",
        "# ارزیابی\n",
        "nn_preds = stacking_nn.predict(X_test)\n",
        "nn_probs = stacking_nn.predict_proba(X_test)\n",
        "\n",
        "# متریک‌ها\n",
        "print(\"\\n🧠 Stacking with Neural Network Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, nn_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, nn_preds, average='weighted'):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, nn_preds))"
      ],
      "metadata": {
        "id": "fDteAlm-ASaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Model**"
      ],
      "metadata": {
        "id": "1mbW3d16VI_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# بارگذاری داده‌ها از فایل اکسل\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "balanced_test_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Text'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "\n",
        "# تبدیل برچسب‌ها به آرایه numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# ایجاد پیش‌بینی‌های تصادفی\n",
        "unique_classes = np.unique(y_train)\n",
        "random_predictions = np.array([random.choice(unique_classes) for _ in range(len(y_test))])\n",
        "\n",
        "# ارزیابی مدل\n",
        "accuracy = accuracy_score(y_test, random_predictions)\n",
        "precision = precision_score(y_test, random_predictions, average='weighted')\n",
        "recall = recall_score(y_test, random_predictions, average='weighted')\n",
        "f1 = f1_score(y_test, random_predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, random_predictions)\n",
        "class_report = classification_report(y_test, random_predictions)\n",
        "\n",
        "# چاپ نتایج ارزیابی\n",
        "print(\"=== Random Baseline Results ===\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# ذخیره نتایج پیش‌بینی شده به فایل اکسل\n",
        "results_df = pd.DataFrame({\n",
        "    'Sentence': balanced_test_texts,\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': random_predictions,\n",
        "    'Prediction Probability': 0.5  # احتمال 50% برای پیش‌بینی تصادفی\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter('random_baseline_results.xlsx') as writer:\n",
        "    results_df.to_excel(writer, index=False)\n",
        "\n",
        "# ترسیم ماتریس درهم‌ریختگی\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['cause-effect', 'others'],\n",
        "            yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Random Baseline')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lgGielsLHF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rule-Based Model**"
      ],
      "metadata": {
        "id": "Y0GyJrE7VM8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# بارگذاری داده‌ها از فایل اکسل\n",
        "balanced_train_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Text'].tolist()\n",
        "balanced_train_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced train data')['Label'].tolist()\n",
        "balanced_test_texts = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Text'].tolist()\n",
        "balanced_test_labels = pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='balanced test data')['Label'].tolist()\n",
        "important_terms = set(pd.read_excel('ACORD_Data_040613.xlsx', sheet_name='anova terms')['Term'].tolist())\n",
        "\n",
        "# تبدیل برچسب‌ها به آرایه numpy\n",
        "y_train = np.array(balanced_train_labels)\n",
        "y_test = np.array(balanced_test_labels)\n",
        "\n",
        "# ==============================================\n",
        "# 1. مدل مبتنی بر کلیدواژه‌های ساده (Rule-Based)\n",
        "# ==============================================\n",
        "print(\"\\n=== Rule-Based Baseline ===\")\n",
        "rule_based_predictions = [\n",
        "    \"cause-effect\" if any(term in text.lower() for term in important_terms)\n",
        "    else \"others\"\n",
        "    for text in balanced_test_texts\n",
        "]\n",
        "\n",
        "# ارزیابی مدل\n",
        "accuracy = accuracy_score(y_test, rule_based_predictions)\n",
        "precision = precision_score(y_test, rule_based_predictions, average='weighted')\n",
        "recall = recall_score(y_test, rule_based_predictions, average='weighted')\n",
        "f1 = f1_score(y_test, rule_based_predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, rule_based_predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "# ترسیم ماتریس درهم‌ریختگی\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['cause-effect', 'others'], yticklabels=['cause-effect', 'others'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wVN070JkHITo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph**"
      ],
      "metadata": {
        "id": "tJBXTC93VRPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import inflect\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = 'all_sentences_with_original.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Extract the entities and labels\n",
        "df['Entity1'] = df['e1']\n",
        "df['Entity2'] = df['e2']\n",
        "\n",
        "# Filter out rows where Entity1 or Entity2 is empty\n",
        "df = df.dropna(subset=['Entity1', 'Entity2'])\n",
        "\n",
        "# Initialize inflect engine\n",
        "p = inflect.engine()\n",
        "\n",
        "# # Normalize entities to singular form\n",
        "def normalize_entity(entity):\n",
        "    entity = str(entity).strip()  # تبدیل مقدار به رشته و حذف فضاهای اضافی\n",
        "    words = entity.split()  # بررسی تعداد کلمات\n",
        "    if len(words) > 1:  # اگر شامل چند کلمه باشد، تغییر ندهیم\n",
        "        return entity\n",
        "    singular = p.singular_noun(entity)\n",
        "    return singular if isinstance(singular, str) else entity  # تغییر فقط در صورت معتبر بودن مقدار\n",
        "\n",
        "# Apply normalization safely\n",
        "df['Entity1'] = df['Entity1'].apply(normalize_entity)\n",
        "df['Entity2'] = df['Entity2'].apply(normalize_entity)\n",
        "\n",
        "# Select relevant columns for Cytoscape and rename columns to source and target\n",
        "cytoscape_df = df[['Entity1', 'Entity2', 'Predicted Label']]\n",
        "cytoscape_df.columns = ['source', 'target', 'label']\n",
        "\n",
        "# Filter out rows where label is 'others'\n",
        "cytoscape_df = cytoscape_df[cytoscape_df['label'] != 'others']\n",
        "\n",
        "# Save the data to a CSV file\n",
        "cytoscape_df.to_csv('cytoscape_data_causal_V040520.csv', index=False)"
      ],
      "metadata": {
        "id": "fz3bQeHeATPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**False Positives and False Negatives**"
      ],
      "metadata": {
        "id": "qIfnQDRVVUmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# خواندن فایل اکسل\n",
        "results_df = pd.read_excel('all_sentences_with_original.xlsx')\n",
        "\n",
        "# تعریف False Positive و False Negative\n",
        "false_positives = results_df[\n",
        "    (results_df['Predicted Label'] == 'cause-effect') &\n",
        "    (results_df['True Label'] == 'others')\n",
        "]\n",
        "print(f\"تعداد کل موارد FP شناسایی شده: {len(false_positives)}\")\n",
        "false_negatives = results_df[\n",
        "    (results_df['True Label'] == 'cause-effect') &\n",
        "    (results_df['Predicted Label'] == 'others')\n",
        "]\n",
        "\n",
        "# ایجاد یک فایل اکسل جدید با تب‌های جداگانه\n",
        "with pd.ExcelWriter('error_analysis.xlsx') as writer:\n",
        "    # ذخیره تمام داده‌ها در تب اول\n",
        "    results_df.to_excel(writer, sheet_name='All Data', index=False)\n",
        "\n",
        "    # ذخیره False Positive در تب دوم\n",
        "    false_positives.to_excel(writer, sheet_name='False Positives', index=False)\n",
        "\n",
        "    # ذخیره False Negative در تب سوم\n",
        "    false_negatives.to_excel(writer, sheet_name='False Negatives', index=False)\n",
        "\n",
        "print(\"✅ فایل error_analysis.xlsx با موفقیت ایجاد شد!\")\n",
        "print(f\"🔍 تعداد False Positives: {len(false_positives)}\")\n",
        "print(f\"🔍 تعداد False Negatives: {len(false_negatives)}\")"
      ],
      "metadata": {
        "id": "IHAnPVcFAg7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}